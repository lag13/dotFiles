Programming Checklist

1. Testing: Did you test the "0" case? The "1" case? And larger cases?

2. Is there any code that has the same structure? If so can you write a
function to encompass that structure.

3. Make your PR's as complete as possible before review

4. Always test ALL types of data that you expect to get. If you expect that
the code you're working on will be tested on a set of data X, then while
you're developing actually see how your code stands up to elements of X. Don't
be lazy and set up the code and only test a couple cases that you manually
created.

5. Test the branch on production before merging the branch and deploying it.

6. When possible, make a function take parameters instead of taking nothing
because it makes it more flexible. If you want the simplicity of calling a
function with no parameters that "just works" then create the parameterized
function first then make the function with no parameters call that function.

7. Before going away for lunch or going on a break think about if you need
anything from other people that doesn't require your intervention and ask for
it. For example, maybe you got a PR ready before lunch, ask someone to review
it before leaving for lunch so by the time you get back there might be
feedback on it.

8. Do the minimal amount of work first, get the minimum viable product both in
terms of the code you write and the actual product. In other words, don't try
to abstract too much from the get go. Even if you think some abstraction might
be useful later, do not do it until you really need it because there is a good
chance that you might never need the abstraction and then when people look at
your code they'll wonder why there is this extra layer.

9. Do a quick mental checklist, and better yet, actually test whether
"special" characters have the potential to break the program. This is
especially important for bash commands.

10. Log as much as possible. If there is an error and that error returns
sensistive information (so we don't want to log it) then instead build as good
of an error message as possible omitting the sensitive info.

When creating logging messages step back and ask yourself, will there be a way
to distinguish this log message from all other logs? If you're logging
something it should have some piece of data associated with it to distinguish
it. That is the point of logs after all, to be able to see what happened long
after the code finished running. In short I think you should think very
carefully before logging any string literals, ALWAYS try to include some piece
of information to make that log unique.

Think about adding info logs before some external call to a service, I just had
an issue with scalp which I think was caused by connection problems but when I
was debugging I had no idea what was going on because an external call was
made, it would hang, then it would time out. If I had an info log just before
that external call things would have been fine.

11. Before rolling out a new change that effects developers (actually anyone I
suppose) . Send out one more message about what is going on and what they need
to do.

12. This advice is coming from using Golang but it probably holds up in other
languages. Never use concrete types inside of a struct/object if you call
methods on those concrete types because it makes it hard to test things. So
instead of a concrete type put an interface in there. Then when it comes to
testing you can make a mock object which satisfies the interface and pass it
in.

13. Testing is hard, its really hard. When writing non-testing code its kind of
easy in the sense that you just have to get things working and if it works then
you're done (barring any refactors and such). But test code is more ambiguous
because you have to deal with questions like: which parts should I test and
which should I not? Is it worth my time to test this feature? If external
services are used, how realistic should the mock be? Then of course there's the
whole problem of changing the existing program so it is testable. That part
always seems hardest because it feels like its another level above getting a
program working. When getting a program working you just really have to worry
about 1. if it works and 2. is it nicely organized. But writing a program with
test code you need 1 and 2 while deciding about what should be tested and how
it should be approached. In short, writing test code seems more of an art than
a science (at least it is for someone more new to it). As I'm writing this I'm
doing feature flagging for TSR (I hope it goes well!) and I'm wrestling with
testing. I think I've pinned down one of the sources of trouble is that when
writing the test code there are more external calls interspersed with the other
logic. When I was writing the test code for snake, there was pretty much just
one external call, which did the drawing. And drawing (at least for snake) had
very little to do with whether the code was behaving correctly. But with
feature flags I've got a bunch of external calls which are really involved with
the surrounding logic. Or maybe its not so much that it is interspersed with
the surrounding logic but that there is a fair amount of logic, involved with
getting the external code to tease out the values we need. So how do I deal
with that? Do I just try to better organize the code so using the external code
is in one package? Then in that package I can test the logic of using that
package. Then in other packages that use that package I can just mock the
overall exported function call in a really simple manner. Yes, I think I like
this. If there is logic involved with the external call then test that logic in
its own package, exposing a nice function for other packages to call. Then if
other packages use that function just mock it and not worry about the internal
representation because that is handled by the package.

I just wanted to say that my above idea of moving all external calls into a
dedicated package really did feel like the correct approach. In other words
when using an external service, put all the external service work in one
package and expose nice functions for other packages to use.

Another thing I've learned from doing this feature flagging stuff is that if
you want to be able to test everything, then anything that returns an error
should be mockable. I guess more generally ANY service you use should be
mockable. For example, I make a call to json.Marshal() and some other json
related functions directly in my code. But since I cannot switch those function
calls out, the logic around them cannot be tested.

I think that when creating interfaces which can be mocked, store those
interfaces as close to the original struct which satisfies them. So in package
a/b you want to create test code which involves mocking calls to a package in
x/y/z. The interface you must satisfy to mock those calls should live in x/y/z
rather than a/b.

14. Always remember to clean up resources after using them. Seems like a silly
thing to notate but sometimes we forget to check that no matter what path the
code takes, that the resource we aquired is cleaned up. Whether it be a socket,
a file, an SQS queue item, a database connection, ... CLEAN IT UP.

15. Construct as many dependencies as you can and then then do the work needed
to be done.

16. I'm thinking that to test concurrency in Go, maybe we never directly make
concurrent requests within the code we test. Maybe we try and treat the
concurrent code like an external service to be mocked. We just make sure we
call that external service as we expect to and then separately test that that
service behaves as we expect it to.

17. Testing is like a rigurous form of documenting how your code is supposed to
behave.

18. If you can get every conditional statement under test coverage then you
should be pretty well off.

19. Generally, libraries should be as general as possible. If you're making a
library for a specific application then do NOT put some constant in the library
if the application can add it instead.

20. Configuration. The only reasons to make something configurable is:

- You expect it might change at some point.
- You need multiple instances of the same service doing slightly different
  things.
- Its something sensitive like usernames and passwords.

That being said, if you do add something which you then can configure, make its
default value make sense for what the application is doing. Hopefully by doing
that you'll never need to use the configuration but if you do, its there.

21. Try to inject behavior you want rather than choose behavior with
conditionals. For example you have an object with a boolean member variable.
You pass in that boolean and if its true you do some computation otherwise you
do different computation. Its better, if possible, to inject the actual
behavior you want into that object instead of the boolean.

22. If you're ever writing a test of some object A and find that some parameter
for A is not important for writing the test then probably A does not need that
parameter, it can be put somewhere else.

23. If you're stuck on a problem, sometimes the best way to solve it is to take
a break from that problem. Sometimes they work themselves out and other times
you just figure it out the next time you sit down.

24. Software is really complicated and sometimes you have no idea what is going
wrong so you just restart the system and maybe watch out for it in the future.
Maybe humans sleep as a sort of "reset". Without sleep our minds would get into
a bad state so we sleep to put us back into a good state. Interesting thoughts.

25. When writing code which seems sort of arbitrary, put a comment on WHY you
wrote the code a certain way. Inspired by TSR. I wrote a pre-upgrade script to
check for certain "bad" configurations before upgrading but I did not run this
script on dev for some reason. I wish I knew the exact reason why we chose to
do that.

26. After making a new change, always check the logs for any errors. Even if
you don't log anything, if a language is dynamic (like PHP) then the language
itself might output logs about type problems and the like. You want to catch
those and solve them. My experience with this is that when I had deployed and
was monitoring my feature flagging stuff I was only looking for problems with
datadog metrics because the only time I logged was when I incrememnted some
sort of "failed" metric. Since I never saw "failed" metrics I thought things
were good. But I had forgot to have a 'use' statement and so one of my
typehints was wrong and there were error logs like this:

	OFCCP: Catchable Fatal Error: Argument 6 passed to
	Luceo\\Bundle\\AppBundle\\Factory\\FeatureFlags\\FeatureFlagsFactory::create()
	must be an instance of
	Luceo\\Bundle\\AppBundle\\Factory\\FeatureFlags\\ExternalServiceMocker,
	instance of Profilsearch\\ExternalServiceMocker\\ExternalServiceMocker given,
	called in
	/media/tmp/legacy/myr-group.luceosolutions.com/cache/prod/appProdProjectContainer.php
	on line 1762 and defined

If I had been mointoring logs (like I should have been) I would have caught
that a long time ago and addressed it.

27. Programming, like many things, is about tradeoffs. Programming especially
involves more tradeoffs when you are working at a larger scale where you have
multiple services calling other services etc... The tradeoffs I'm talking about are:

- Getting the "perfect" solution. Where perfect means that the code is abstract
  and well tested and just broken up very nicely AND maybe the code satisfies
  some other higher goals like having each datacenter be able to work
  independently if one goes down.
- Getting something out there that works even if it is not perfect.

It might be a good idea to accept the fact that a solution you come up with
will probably never be perfect. Even if you think of the perfect solution, if
it will be hard to implement/maintain then maybe that is not the solution to go
with (like maybe the logic is complicated and that makes it hard to change).
The inspiration for this bullet was the feature flagging service I was building
for TSR. We were wondering how to architect the solution. Originally we were
going to have this service called scalp on each of the 4 datacenters which set
the flags in mysql (which is considered they authoritative source of feature
flagging data) and then in redis which the application uses. We realized that
each of these mysql databases would have the same exact information in them
which felt weird. It would also be hard to synchronize data between them
because we now have 4 mysql databases so 4 authoritative sources of data.
Synchronizing data would involve some other process pinging all the databases
and determining which values should be the flag values and then telling the 4
scalp instances what the values should be. This seems complicated to say the
least. So we thought that moving the database into the UX/API would be the
right thing to do. Then making sure data is consistant between redis and mysql
is easier because we only have one source of feature flagging data. But the
thing that we didn't really like about that design was that the datacenters are
not independent now. So if the UX/API was on the dev datacenter and dev went
down somehow then we could not do any feature flagging stuff. I also started
wondering how companies like facebook and google have a website which is
accessed globally. There's no way that they have a server in one region which
serves the website because that would make it slower for that person. Like if
the server was in the US then someone in Japan would have a tougher time using
google because it would be slower. Anyway, we decided that the tradeoff of
being reliant on a single datacenter was a lower cost than developing this
complicated algorithm/process for making sure the feature flagging data is
consistant between all datacenters. What we sacrifice is datacenter
independence but we might not have that anyway so the issue might not be so
great. I guess the only situation that would be bad is if dev went down (but
not US or EU) and a feature was misbehaving then we would not be able to turn
off that feature. Strictly speaking though we could, we'd just have to get into
the redis instance and manually turn it off. Not a great solution but at least
we do have some sort of way out in case that happens. But yeah to sum up:

- We had a choice between simple algorithm or database independence
- We chose having the simple algorithm because we thought the cost of
  developing that algorithm (which involves actually coming up with said
  algorithm and then spending the time and energy to make sure it works) is
  higher than the cost of not having datacenter independence. Especially since
  if a datacenter goes down, this service being down won't bring down TSR. TSR
  is probably the thing which should be super fault tolerant, not this
  application. In general the customer facing application should probably be
  the thing that is the most stable.

Again, I guess just try to remember that as much as we'd like programming to be
this exact science where there is one truth, one best solution, and we just
need to think about things long enough to find that best solution, it is not
like that. Programming is partly about that, definitely. Focusing on that
aspect gives us clean code that is a pleasure to work with and extend! But keep
in mind that there is also a business side to programming. This is the part of
programming which says, "we need to build something which works, which we can
sell". We might like the idea of coming up with the "perfect" solution for a
particular problem which is abstract and flexible and easy to maintain and all
of that good stuff but you need to figure out the pros and cons between getting
to that perfect solution or simplifying and getting something that works more
quickly. If you have the theoretical idea for a perfect solution but don't know
exactly how to implement it, then maybe it should be abandoned for a simpler
solution. Because coming up with the idea soution is not enough, you have to
now go and implement said solution which takes time, bugs will occurr and will
have to be fixed. This all takes time out of your day, time which could be
spent solving other problems. Of course I'm not advocating to always be
programming with the mindset of "I just need to get something that works"
because you'll inevitably end up doing a bunch of "hacky" as time goes on it
will be harder and harder to make changes i.e you'll accumulate technichal
debt.

Tradeoffs tradeoffs, tradeoffs.

28. If something feels too hard to test then maybe it is and instead of trying
to create the correct set of inputs which get the coverage you want (which
usually involves a lot of hacky logic where you say things like, "if value A is
this and value B is this then return this otherwise if value A is this then do
this other thing") create a new object which abstracts away some of that logic
behind a simple interface. Then you can have a mock implementation of that
interface and your tests will be simpler. Of course you now have to create the
production focused implementation of that interface (and perhaps test it too)
but that should be an easier task since the domain you're testing is simpler.

If something is still too hard to test, then (if possible) try to change the
problem so it is easier to solve then solve and test that. And sometimes I'm
sure that testing something will be hard period, so just suck it up and deal
with it.

29. Creating something yourself or using an existing product pros and cons.
Pros: Developers practice craft, you can extend it to suit companies needs if
need be, you might get other useful tools out of it in the process. Cons: It
takes time (which also means money) which could be spent developing elsewhere,
your homemade product will probably never be as good as one produced by a
company who's whole purpose is to perfect that produc.

30. Today (08/14/16, a Sunday) I was lying in bed next to Ara, who was reading
harry potter book 4, in 40 east 9th streat Chicago, in good old apartment 1612
thinking about nothing in particular. Pete was going to take a
html/css/javascript related test later in the day sent to him by some company
he had applied to and he had been studying those topics in preparation. At some
point he walked over to Kye and I thought he said something to the effect of,
"It feels like all problems have already been solved. If I don't know the
answer I just look it up and there's the solution. It feels more about
implementation than actually solving problems". I had felt this sort of feeling
before and it kind of got me down at the time but I remember coming to terms
with it but never put it into words. I didn't even remember exactly how I had
come to terms with it so it got me thinking for a little while about why
programming isn't so bleak as that. Because to me, thinking about programming
in that way does feel bleak. When I think in such a way (about anything really)
then I start to think things like, "if someone has already solved this problem,
then what am I really doing? I'm just copying their solution and adding it to
my code. I'm like a monkey, who doesn't even think but instead takes input
describing a problem, looks it up, and copies the solutions find.". Here are a
couple of reasons why things are not so bleak:

- You can still solve things yourself, just don't look up how to solve them and
  figure them out! It might still bum you out that the problem is already
  solved but at least from your perspective it is new.
- Looking up a solution (unless it is very simple) is almost never as easy as a
  copy paste. You have to read the code you're seeing and understand it to be
  able to say whether or not it can be applied to what you want to do. As you
  do that more you inevitably learn more things about the craft which can be
  applied to future situations. You are still a lot more than a mere monkey.
- Lastly (and this reason resonates with me the most) I would equate
  programming/solving some problem in programming to creating something for
  aesthetic reasons (painting, drawing, wood burning, etc...). Hundreds of
  thousands (millions really) have, say, drawn/painted a fruit basket.
  Producing something that represents the fruit basket is "solving the problem"
  in this metaphor while how you went about doing it (the medium, personal
  style, etc...) is the implementation. So many people have already "solved"
  this problem but I get the feeling that that doesn't deter people from doing
  it themselves. Because in many ways, its not really about the end product,
  its about how *you* decide to implement it. Its about what experiences you
  bring to the table and what you personally are trying to accomplish (in terms
  of what the "bowl of fruit" looks like) and how those translate into the
  final product. Not to mention you get more practice at your craft. Rarely am
  I satisfied from simply solving the problem, there's an initial thrill of
  satisfaction but what really makes me happy is when the code behind my
  solution is what I consider good. In the metaphor this would be drawing the
  basket of fruit but drawing it badly or not getting the style across you were
  going for. The metaphor kind of breaks down here though because it is easy to
  make a program which solves the problem but is written badly but if the "bowl
  of fruit" does not look good to you then you are not done. For programming,
  the act of solving the problem is more decoupled from the aesthetics of the
  solution where for various art forms, the aesthetic IS the solution.
  Regardless of the metaphor, the idea here is that even though the problem is
  "solved" and you are just implementing it, there is still a lot of creativity
  needed to get the code to a state that makes you happy. Getting the code to
  be aesthetically pleasing to me is probably my main goal, solving the problem
  at hand is just a byproduct.
- If it makes you feel better, you can take some solace in the fact that
  probably most people are not solving truly novel problems. What's so
  inherently good about solving a novel problem anyway?

31. Just realized another benefit for having tests. Sometimes a software
project just gets so large that knowing if a change you are making effects
other pieces is hard to judge. You could ask co-workers who know the software
more than you but what if they are busy? If your tests are good then they will
fail if you made a change which broke something. The inspiration for this one
came on 08/17/16, we wanted to use an existing key in platform.yml called
'region'. The strange thing was though was that this key was set to 'us' on
every datacenter where we wanted it set to things like 'dev', 'us', 'eu' etc...
Looking through the code I found only one reference to this key and it was a
hardcoded check to see if the key was equal to 'us', very strange. Anyway, I
was worried about removing that check because I wasn't sure if there would be
breaking changes. If there were great tests in place, I wouldn't need to worry.
